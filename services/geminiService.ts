import { GoogleGenAI, Type } from "@google/genai";
import { ChatMessage, ProductSuggestion } from "../types";

// Helper to init AI
const getAI = () => new GoogleGenAI({ apiKey: process.env.API_KEY });

/**
 * Reimagines a room photo in a specific style using Gemini.
 */
export const generateReimaginedRoom = async (
  imageBase64: string,
  style: string,
  customInstruction?: string
): Promise<string> => {
  const ai = getAI();
  const prompt = customInstruction 
    ? `Redesign this room. ${customInstruction}`
    : `Redesign this room in a ${style} interior design style. Keep the structural layout (walls, windows, doors) exactly the same, but replace furniture, colors, and decor to match the ${style} aesthetic. Photorealistic, 8k resolution, interior design magazine quality.`;

  try {
    // Using gemini-2.5-flash-image for reasonably fast generation with image input support
    const response = await ai.models.generateContent({
      model: 'gemini-2.5-flash-image',
      contents: {
        parts: [
          {
            inlineData: {
              mimeType: 'image/jpeg',
              data: imageBase64,
            },
          },
          { text: prompt },
        ],
      },
      config: {
        // We use generateContent for nano-banana models which returns text/image parts
        // No strict JSON schema needed here, we just want the image.
      }
    });

    // Extract image
    if (response.candidates?.[0]?.content?.parts) {
      for (const part of response.candidates[0].content.parts) {
        if (part.inlineData && part.inlineData.data) {
          return `data:image/jpeg;base64,${part.inlineData.data}`;
        }
      }
    }
    throw new Error("No image generated by the model.");
  } catch (error) {
    console.error("Image generation failed:", error);
    throw error;
  }
};

/**
 * Chat with the design consultant. Uses Search Grounding for products.
 */
export const chatWithConsultant = async (
  history: ChatMessage[],
  currentMessage: string,
  currentImageBase64?: string
): Promise<{ text: string; products: ProductSuggestion[] }> => {
  const ai = getAI();
  
  // Prepare history for context
  // We only take the last few turns to save tokens and keep context relevant
  const recentHistory = history.slice(-6).map(msg => ({
    role: msg.role,
    parts: [{ text: msg.text }]
  }));

  const parts: any[] = [{ text: currentMessage }];
  
  // If there's an active image being looked at, send it for context
  // (We attach it to the user's latest message)
  if (currentImageBase64) {
    parts.unshift({
      inlineData: {
        mimeType: 'image/jpeg',
        data: currentImageBase64
      }
    });
  }

  try {
    const response = await ai.models.generateContent({
      model: 'gemini-3-flash-preview', // Good balance of reasoning + search
      contents: [
        ...recentHistory,
        { role: 'user', parts: parts }
      ],
      config: {
        systemInstruction: "You are LUMA, an expert high-end interior design consultant. Help the user refine their room design. If they ask for products, furniture, or decor items, use Google Search to find real, purchaseable items. Be concise, helpful, and stylish.",
        tools: [{ googleSearch: {} }] // Enable search for grounding
      }
    });

    const text = response.text || "I'm having trouble connecting to the design database.";
    
    // Parse grounding chunks for product links
    const products: ProductSuggestion[] = [];
    const groundingChunks = response.candidates?.[0]?.groundingMetadata?.groundingChunks;

    if (groundingChunks) {
      groundingChunks.forEach((chunk: any) => {
        if (chunk.web && chunk.web.uri && chunk.web.title) {
          products.push({
            title: chunk.web.title,
            url: chunk.web.uri,
            source: chunk.web.source || 'Web'
          });
        }
      });
    }

    return { text, products };

  } catch (error) {
    console.error("Chat failed:", error);
    return { text: "I'm sorry, I'm having trouble processing that request right now.", products: [] };
  }
};
